from curl_cffi import requests # ä½¿ç”¨æ”¯æ´æ¨¡æ“¬æŒ‡ç´‹çš„ requests
import pandas as pd
import os
import sys
from datetime import datetime
from pathlib import Path
import time
import random

# -----------------------------------------------------
# è¨­å®š
# -----------------------------------------------------
DATA_DIR = Path("data")
CSV_PATH = DATA_DIR / "SCORE.csv"

# åœ‹ç™¼æœƒé¦–é 
PAGE_URL = "https://index.ndc.gov.tw/n/zh_tw/data/eco"
# è³‡æ–™ API
API_URL = "https://index.ndc.gov.tw/n/json/data/economy/indicator"

# Headers è¨­å®š
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36",
    "Accept": "application/json, text/javascript, */*; q=0.01",
    "Referer": "https://index.ndc.gov.tw/n/zh_tw/data/eco",
    "X-Requested-With": "XMLHttpRequest",
    "Origin": "https://index.ndc.gov.tw"
}

def fetch_score_data():
    print("ðŸš€ [Job: Score] é–‹å§‹æŠ“å–åœ‹ç™¼æœƒæ™¯æ°£å°ç­–ä¿¡è™Ÿ (curl_cffi + GET)...")

    # 1. ç¢ºä¿è³‡æ–™å¤¾å­˜åœ¨
    DATA_DIR.mkdir(parents=True, exist_ok=True)

    try:
        # 2. åˆå§‹åŒ– Session (impersonate="chrome110")
        s = requests.Session(impersonate="chrome110") 
        s.headers.update(HEADERS)

        # [æ­¥é©Ÿ A] é€ è¨ªé¦–é å–å¾— Cookie
        print(f"   ...æ­£åœ¨é€ è¨ªé é¢: {PAGE_URL}")
        r1 = s.get(PAGE_URL, timeout=15)
        
        # ä¼‘æ¯ä¸€ä¸‹
        time.sleep(random.uniform(1, 2))

        # [æ­¥é©Ÿ B] è«‹æ±‚ API (æ”¹æˆ GET)
        print("   ...æ­£åœ¨è«‹æ±‚è³‡æ–™ API")
        
        # åƒæ•¸ï¼šsys=10(æ™¯æ°£), cat=15(ç‡ˆè™Ÿ), ind=74(åˆ†æ•¸)
        payload = {'sys': 10, 'cat': 15, 'ind': 74}
        
        # ã€é—œéµä¿®æ­£ã€‘é€™è£¡æ”¹æˆ getï¼Œä¸¦ä¸”ç”¨ params å‚³éžåƒæ•¸
        res = s.get(API_URL, params=payload, timeout=15)
        
        # æª¢æŸ¥å›žæ‡‰
        if res.status_code != 200:
            print(f"âŒ API å›žæ‡‰éŒ¯èª¤: {res.status_code}")
            print(f"   å›žæ‡‰å…§å®¹: {res.text[:200]}") 
            sys.exit(1)
            
        data = res.json()
        
    except Exception as e:
        print(f"âŒ [Job: Score] é€£ç·šç™¼ç”Ÿä¾‹å¤–ç‹€æ³: {e}")
        sys.exit(1)

    # 3. è§£æž JSON
    target_data = None
    if isinstance(data, dict):
         for key, val in data.items():
            if isinstance(val, dict) and "lines" in val:
                for line in val["lines"]:
                    title = line.get("title", "")
                    if "æ™¯æ°£å°ç­–ä¿¡è™Ÿ" in title and "(åˆ†)" in title:
                        target_data = line["data"]
                        break
            if target_data: break
            
    if not target_data:
        print("âŒ [Job: Score] æ‰¾ä¸åˆ°è³‡æ–™ (API çµæ§‹å¯èƒ½æ”¹è®Š)")
        # å°å‡º Key ä¾†é™¤éŒ¯
        print(f"DEBUG Keys: {list(data.keys()) if isinstance(data, dict) else data}")
        sys.exit(1)

    # 4. æ•´ç†æ•¸æ“š
    records = []
    print(f"   ...å–å¾— {len(target_data)} ç­†è³‡æ–™ï¼Œæ­£åœ¨æ•´ç†...")
    for item in target_data:
        try:
            raw_date = str(item['x'])  # "198401"
            score = item['y']
            
            dt_obj = datetime.strptime(raw_date, "%Y%m")
            fmt_date = dt_obj.strftime("%Y-%m-%d")
            records.append({"Date": fmt_date, "Score": score})
        except:
            continue

    if not records:
        print("âŒ [Job: Score] ç„¡æœ‰æ•ˆæ•¸æ“š")
        sys.exit(1)

    # 5. å­˜æª”
    df = pd.DataFrame(records)
    df = df.set_index("Date")
    df = df.sort_index()
    
    df.to_csv(CSV_PATH)
    print(f"âœ… [Job: Score] æ›´æ–°å®Œæˆï¼å·²å„²å­˜è‡³: {CSV_PATH}")
    print(f"   æœ€æ–°åˆ†æ•¸: {df.index[-1]} -> {df['Score'].iloc[-1]} åˆ†")

if __name__ == "__main__":
    fetch_score_data()
