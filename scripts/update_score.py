from curl_cffi import requests # é—œéµï¼šä½¿ç”¨ curl_cffi ç¹é TLS æŒ‡ç´‹åµæ¸¬
import pandas as pd
import os
import sys
from datetime import datetime
from pathlib import Path
import time
import random

# -----------------------------------------------------
# è¨­å®š
# -----------------------------------------------------
DATA_DIR = Path("data")
CSV_PATH = DATA_DIR / "SCORE.csv"

# éƒ¨è½æ ¼æåˆ°çš„ API ç¶²å€ (å€‰åº«å…¥å£)
API_URL = "https://index.ndc.gov.tw/n/json/data/economy/indicator"
# åœ‹ç™¼æœƒé¦–é  (ç”¨ä¾†æ‹¿é€šè¡Œè­‰)
PAGE_URL = "https://index.ndc.gov.tw/n/zh_tw/data/eco"

# å½è£æˆ Chrome ç€è¦½å™¨
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Origin": "https://index.ndc.gov.tw",
    "Referer": "https://index.ndc.gov.tw/n/zh_tw/data/eco"
}

def fetch_score_data():
    print("ğŸš€ [Job: Score] é–‹å§‹åŸ·è¡Œ (éƒ¨è½æ ¼ API æ–¹æ³• + curl_cffi å½è£)...")
    DATA_DIR.mkdir(parents=True, exist_ok=True)

    try:
        # 1. åˆå§‹åŒ–å½è£ Session (æ¨¡æ“¬ Chrome 110)
        s = requests.Session(impersonate="chrome110")
        s.headers.update(HEADERS)

        # [æ­¥é©Ÿ A] å…ˆå»é¦–é æ™ƒä¸€ä¸‹ï¼Œæ‹¿ Cookie (è£å¾—åƒçœŸäºº)
        print(f"   ...æ­£åœ¨é€ è¨ªé¦–é å–å¾— Cookie: {PAGE_URL}")
        s.get(PAGE_URL, timeout=15)
        time.sleep(random.uniform(1, 2)) # ä¼‘æ¯ä¸€ä¸‹

        # [æ­¥é©Ÿ B] ç™¼é€ POST è«‹æ±‚ (é€™æ˜¯éƒ¨è½æ ¼çš„æ ¸å¿ƒæ­¥é©Ÿ)
        print("   ...æ­£åœ¨ç™¼é€ POST è«‹æ±‚è‡³ API")
        
        # é€™æ˜¯éƒ¨è½æ ¼æ–‡ç« ä¸­æåˆ°çš„é—œéµåƒæ•¸
        payload = {
            'sys': 10,  # æ™¯æ°£æŒ‡æ¨™
            'cat': 15,  # æ™¯æ°£å°ç­–ä¿¡è™Ÿ
            'ind': 74   # åˆ†æ•¸
        }
        
        # ä½¿ç”¨ POST (å› ç‚º GET æœƒå›å‚³ 405)
        res = s.post(API_URL, data=payload, timeout=15)
        
        # æª¢æŸ¥å›æ‡‰
        if res.status_code != 200:
            print(f"âŒ API å›æ‡‰éŒ¯èª¤: {res.status_code}")
            print(f"   å›æ‡‰å…§å®¹: {res.text[:200]}")
            sys.exit(1)
            
        data = res.json()
        print("   âœ… æˆåŠŸå–å¾— JSON è³‡æ–™ï¼")

    except Exception as e:
        print(f"âŒ é€£ç·šå¤±æ•—: {e}")
        sys.exit(1)

    # 3. è§£æè³‡æ–™ (åƒè€ƒéƒ¨è½æ ¼çš„è§£æé‚è¼¯)
    target_data = None
    
    # åœ‹ç™¼æœƒ API å›å‚³çµæ§‹é€šå¸¸åœ¨ lines è£¡é¢
    # æˆ‘å€‘éæ­·å°‹æ‰¾æ¨™é¡ŒåŒ…å« "æ™¯æ°£å°ç­–ä¿¡è™Ÿ" ä¸”åŒ…å« "(åˆ†)" çš„æ•¸æ“š
    if isinstance(data, dict):
         for key, val in data.items():
            if isinstance(val, dict) and "lines" in val:
                for line in val["lines"]:
                    title = line.get("title", "")
                    if "æ™¯æ°£å°ç­–ä¿¡è™Ÿ" in title and "(åˆ†)" in title:
                        target_data = line["data"]
                        break
            if target_data: break
            
    if not target_data:
        print("âŒ æ‰¾ä¸åˆ°ç›®æ¨™æ•¸æ“š (API çµæ§‹å¯èƒ½èˆ‡éƒ¨è½æ ¼æ–‡ç« ä¸åŒ)")
        sys.exit(1)

    # 4. æ•´ç†æ•¸æ“š
    records = []
    print(f"   ...æ­£åœ¨æ•´ç† {len(target_data)} ç­†æ•¸æ“š...")
    
    for item in target_data:
        try:
            # item['x'] æ˜¯æ—¥æœŸ (å¦‚ 202401)
            # item['y'] æ˜¯åˆ†æ•¸ (å¦‚ 27)
            raw_date = str(item['x'])
            score = item['y']
            
            dt_obj = datetime.strptime(raw_date, "%Y%m")
            fmt_date = dt_obj.strftime("%Y-%m-%d")
            
            records.append({"Date": fmt_date, "Score": score})
        except:
            continue

    if not records:
        print("âŒ è§£æå¾Œç„¡æœ‰æ•ˆæ•¸æ“š")
        sys.exit(1)

    # 5. å­˜æª”
    df = pd.DataFrame(records)
    df = df.set_index("Date")
    df = df.sort_index()
    
    df.to_csv(CSV_PATH)
    print(f"ğŸ‰ [Job: Score] æ›´æ–°å®Œæˆï¼å·²å„²å­˜è‡³: {CSV_PATH}")
    print(f"   è³‡æ–™å€é–“: {df.index[0]} ~ {df.index[-1]}")
    print(f"   æœ€æ–°åˆ†æ•¸: {df['Score'].iloc[-1]} åˆ†")

if __name__ == "__main__":
    fetch_score_data()
